{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "aujtRZKSczZC",
    "outputId": "49185a30-e2a6-44fc-d69e-f897d2254709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from torch.autograd import grad\n",
    "from torch.nn import Parameter\n",
    "cuda = torch.cuda.is_available()\n",
    "import pickle as pk\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from torchvision.transforms import transforms\n",
    "from typing import Union,Tuple\n",
    "\n",
    "\n",
    "import fish_models\n",
    "from fish_models import utils\n",
    "import robofish.io\n",
    "\n",
    "fishes = 2\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfilename = 'model.pth'\n",
    "\n",
    "# Test training\n",
    "n_training_iterations = 10\n",
    "n_files = 1\n",
    "n_timesteps = 1500\n",
    "n_speed_bins = 21\n",
    "n_turn_bins = 21\n",
    "n_view_bins = 5\n",
    "n_timesteps_simulation = 2000\n",
    "batch_size = 64\n",
    "\n",
    "\"\"\"\n",
    "# Full training\n",
    "n_training_iterations = 5000\n",
    "n_files = None\n",
    "n_timesteps = None\n",
    "n_speed_bins = 63\n",
    "n_turn_bins = 63\n",
    "n_view_bins = 31\n",
    "n_timesteps_simulation = 10000\n",
    "batch_size = 64\n",
    "\"\"\"\n",
    "\n",
    "# IO Files in cm, actions in m/s\n",
    "data_folder = Path(\"data/live_female_female/train\")\n",
    "test_data_folder = Path(\"data/live_female_female/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qajbJVVEJzbX",
    "outputId": "aa6df4ad-cb55-4ea9-bf48-2663a072570a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetBlock(\n",
       "  (conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(64, 16, kernel_size=(3,), stride=(2,), padding=(6,))\n",
       "  (relu4): LeakyReLU(negative_slope=0.01)\n",
       "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (identity_upsample_first): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (identity_upsample_second): Sequential(\n",
       "    (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet uses blocks\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        # structure\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels= 16, kernel_size=self.kernel_size, stride=1, padding=1)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=self.kernel_size, stride=1, padding=1)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=self.kernel_size, stride=1, padding=1)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=16, kernel_size=self.kernel_size, stride=2, padding=6)\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "\n",
    "        # 1x1 conv filters can be used to change the dimensionality in the filter space.\n",
    "        self.identity_upsample_first = nn.Sequential(nn.Conv1d(in_channels=16, out_channels=32, kernel_size=1, stride=1),\n",
    "                                          nn.BatchNorm1d(32))\n",
    "        self.identity_upsample_second = nn.Sequential(nn.Conv1d(in_channels=32, out_channels=16, kernel_size=1, stride=1),\n",
    "                                          nn.BatchNorm1d(16))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we use this block multipy times\n",
    "        \n",
    "        # save first identity\n",
    "        identity1 = x.clone()\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)      \n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # sum point\n",
    "        # 1. Upsample identity size\n",
    "        identity1 = self.identity_upsample_first(identity1)\n",
    "        # 2. Summarize (skip connection)\n",
    "        x = x + identity1\n",
    "        # 3. save second identity and change size to be able to summurize in the next sum point\n",
    "        identity_upsampled = self.identity_upsample_second(x)\n",
    "   \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        # sum point\n",
    "        x = x + identity_upsampled\n",
    "        return x\n",
    "\n",
    "block = ResNetBlock(in_channels=1, out_channels=1)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dL9m_NKSORzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extern loop START\n",
      "Extern loop END\n",
      "Extern loop START\n",
      "Extern loop END\n",
      "Extern loop START\n",
      "Extern loop END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu1): LeakyReLU(negative_slope=0.01)\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu3): LeakyReLU(negative_slope=0.01)\n",
       "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv4): Conv1d(64, 16, kernel_size=(3,), stride=(2,), padding=(6,))\n",
       "      (relu4): LeakyReLU(negative_slope=0.01)\n",
       "      (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (identity_upsample_first): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (identity_upsample_second): Sequential(\n",
       "        (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu1): LeakyReLU(negative_slope=0.01)\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu3): LeakyReLU(negative_slope=0.01)\n",
       "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv4): Conv1d(64, 16, kernel_size=(3,), stride=(2,), padding=(6,))\n",
       "      (relu4): LeakyReLU(negative_slope=0.01)\n",
       "      (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (identity_upsample_first): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (identity_upsample_second): Sequential(\n",
       "        (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu1): LeakyReLU(negative_slope=0.01)\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu2): LeakyReLU(negative_slope=0.01)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (relu3): LeakyReLU(negative_slope=0.01)\n",
       "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv4): Conv1d(64, 16, kernel_size=(3,), stride=(2,), padding=(6,))\n",
       "      (relu4): LeakyReLU(negative_slope=0.01)\n",
       "      (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (identity_upsample_first): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (identity_upsample_second): Sequential(\n",
       "        (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_end): Conv1d(16, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (flt_end): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc_end): Linear(in_features=10, out_features=42, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 2d: [batch_size, num_features (aka: C * H * W)]\n",
    "# use for nn.Linear() input.\n",
    "\n",
    "class ResNet(nn.Module): \n",
    "    # layers is the list telling us how much to reuse the block in each block\n",
    "    def __init__(self, block, N, batch_size):\n",
    "        \"\"\"ResNet is a short name for Residual Network.\n",
    "        A deep convolutional neural network, \n",
    "        several layers are stacked and are trained to the task at hand. \n",
    "        The network learns several low/mid/high level features at the end of its layers. \n",
    "        \n",
    "        The main innovation of ResNet is the skip connection. \n",
    "        Deep networks often suffer from vanishing gradients, \n",
    "        ie: as the model backpropagates, the gradient gets smaller and smaller. \n",
    "        \n",
    "        This allows to stack additional layers and build a deeper network.\n",
    "        \n",
    "        Args:\n",
    "            TODO\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 1\n",
    "        self.kernel_size = 3\n",
    "        self.start_identity = None\n",
    "        self.batch_small = 2\n",
    "        self.N = N\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Begin\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels= 16, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        # nn.BatchNorm2d 4 expects 4D inputs in shape of [batch, channel, height, width]\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "\n",
    "        # ResNet Layers\n",
    "        # repeat 3 times for i in range(2)\n",
    "        # out_channel is equal to 16 * 2 * i => 16*2^0, 16*2^1, 16*2^2 => 16, 32 and 64 out_channels\n",
    "        self.layer1 = self.make_layer(block, self.N, out_channels=2)\n",
    "        self.layer2 = self.make_layer(block, self.N, out_channels=4)\n",
    "        self.layer3 = self.make_layer(block, self.N, out_channels=8)\n",
    "\n",
    "        # End\n",
    "        self.conv_end = nn.Conv1d(in_channels=16, out_channels= 1, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.flt_end = nn.Flatten()\n",
    "        self.fc_end = nn.Linear(in_features=10, out_features=n_speed_bins + n_turn_bins)\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        # Begin\n",
    "        # [batch_size, seq_length] as input data -> add the channel dimension using unsqueeze\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        # ResNet Layers forward step\n",
    "        # loops inside every layer\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # end of inner blocks\n",
    "\n",
    "        # Tail of the ResNet\n",
    "        x = self.conv_end(x)\n",
    "        x = self.flt_end(x)\n",
    "        x = self.fc_end(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def make_layer(self, block, num_residual_blocks, out_channels):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        print(f'Extern loop START')\n",
    "        layers.append(block(self.in_channels, out_channels))\n",
    "        self.in_channels = out_channels * 2\n",
    "\n",
    "          # intern loops for layers and skip connections\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "            print(f'Inner layer {n} is created!')\n",
    "\n",
    "        print(f'Extern loop END')\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "d_mod = ResNet(ResNetBlock, N=1, batch_size=batch_size)\n",
    "d_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "JT6EkAY--Ltv",
    "outputId": "ba84add5-4a30-4635-9b10-5013b8cce70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extern loop START\n",
      "Extern loop END\n",
      "Extern loop START\n",
      "Extern loop END\n",
      "Extern loop START\n",
      "Extern loop END\n"
     ]
    }
   ],
   "source": [
    "class ResNetFishModel(fish_models.gym_interface.AbstractModel):\n",
    "    def __init__(self, speed_bins, turn_bins):\n",
    "        \"\"\"ResNet (Residual Network) deep convolutional neural network, \n",
    "        \n",
    "        Args:\n",
    "            speed_bins: The array with the float borders between speed bins\n",
    "            turn_bins: The array with the float borders between turn bins\n",
    "        \"\"\"\n",
    "        self.speed_bins = speed_bins\n",
    "        self.turn_bins = turn_bins\n",
    "        \n",
    "        self.deep_model = ResNet(ResNetBlock, N=1, batch_size=batch_size)\n",
    "        \n",
    "    def predict_proba(self, view: np.ndarray):\n",
    "        tensor_proba = torch.tensor(np.array([view])).float()\n",
    "        print(tensor_proba)\n",
    "        print(tensor_proba.shape)\n",
    "        tensor_proba = tensor_proba.squeeze(1)\n",
    "        print(tensor_proba)\n",
    "        print(tensor_proba.shape)\n",
    "\n",
    "        #output = self.deep_model(torch.tensor(np.array([view]).astype(np.double)))\n",
    "        output = self.deep_model(tensor_proba)\n",
    "\n",
    "        return output.detach().numpy() \n",
    "        \n",
    "    def choose_action(self, view: np.ndarray) -> Tuple[float, float]:\n",
    "        probabilities = self.predict_proba([view])\n",
    "        speed_probabilities = probabilities[0, : len(self.speed_bins)]\n",
    "        turn_probabilities = probabilities[0, len(self.speed_bins) :]\n",
    "\n",
    "        speed_probabilities += np.abs(np.min(speed_probabilities))\n",
    "        turn_probabilities += np.abs(np.min(turn_probabilities))\n",
    "\n",
    "        speed_probabilities /= np.sum(speed_probabilities)\n",
    "        turn_probabilities /= np.sum(turn_probabilities)\n",
    "\n",
    "        speed = np.random.choice(self.speed_bins, p=speed_probabilities)\n",
    "        turn = np.random.choice(self.turn_bins, p=turn_probabilities)\n",
    "\n",
    "        # Double sampling (sample inside the chosen bin)\n",
    "        speed += np.random.random() * np.diff(self.speed_bins)[0]\n",
    "        turn += np.random.random() * np.diff(self.turn_bins)[0]\n",
    "\n",
    "        return [speed, turn]\n",
    "    \n",
    "\n",
    "    def train(self, dset, test_dset, optimizer, criterion, max_epochs):\n",
    "       \n",
    "        \"\"\"Binarize the binned actions and train the classifier\n",
    "\n",
    "        Args:\n",
    "            dset: An IoDataset with at least output_strings [\"actions_binned\", \"views\"]\n",
    "        \"\"\"\n",
    "\n",
    "        self.poses_storage = []\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                dset,\n",
    "                collate_fn=fish_models.datasets.io_dataset.IODatasetPytorchDataloaderCollateFN(\n",
    "                    [\"views\", \"actions_binned\"], [torch.float64, torch.long]\n",
    "                ),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                test_dset,\n",
    "                collate_fn=fish_models.datasets.io_dataset.IODatasetPytorchDataloaderCollateFN(\n",
    "                    [\"views\", \"actions_binned\"], [torch.float64, torch.long]\n",
    "                ),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "        \n",
    "        \n",
    "        losses = []\n",
    "        mean_losses = []        \n",
    "        batch_total = len(train_loader)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            samples_total = 0\n",
    "            samples_correct = 0\n",
    "            test_samples_total = 0\n",
    "            test_samples_correct = 0\n",
    "            for batch_idx,batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                x, y = batch\n",
    "\n",
    "                if cuda:\n",
    "                    x, y = x.cuda(),y.cuda()\n",
    "\n",
    "                output = model.deep_model(x)\n",
    "                loss = criterion(output[:,:n_speed_bins], y[:,0]) + criterion(output[:,n_speed_bins:], y[:,1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                yhat1 = torch.argmax(output[:,:n_speed_bins], dim = 1)\n",
    "                yhat2 = torch.argmax(output[:,n_speed_bins:], dim = 1)\n",
    "                samples_total = 2*len(y)\n",
    "                samples_correct += torch.sum(yhat1 == y[:,0])\n",
    "                samples_correct += torch.sum(yhat2 == y[:,1])\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                mean_losses.append(np.mean(losses))\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    acc = float(samples_correct)/float(samples_total)\n",
    "                    \n",
    "                sys.stdout.write(f'\\rEpoch: {epoch:2}/{max_epochs:2} Step: {batch_idx:2}/{batch_total:2} Loss: {loss.item():10.6f} ')\n",
    "\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.deep_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'losses': losses\n",
    "            }\n",
    "            self.savemodel(checkpoint)\n",
    "        \n",
    "        print('\\nFinished Training')\n",
    "\n",
    "        return losses, mean_losses\n",
    "    \n",
    "    def savemodel(self,checkpoint):    \n",
    "        checkpoint_path = os.path.join(modelfilename)\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pk.dump(checkpoint, f)\n",
    "\n",
    "\n",
    "\"\"\" Find the 0.5% percentile and 99.5% percentile of the data to set the bins. \"\"\"\n",
    "\n",
    "# Create the bins. The values come from the quantiles and testing.\n",
    "speed_bins = np.linspace(-2, 20, n_speed_bins)\n",
    "turn_bins = np.linspace(-10, 10, n_turn_bins)\n",
    "\n",
    "# ! CREATE MODEL !\n",
    "\n",
    "model = ResNetFishModel(speed_bins, turn_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RG3tZv6T-M00"
   },
   "outputs": [],
   "source": [
    "raycast = fish_models.gym_interface.Raycast(\n",
    "            n_wall_raycasts=n_view_bins,\n",
    "            n_fish_bins=n_view_bins,\n",
    "            fov_angle_fish_bins= 2 * np.pi,\n",
    "            fov_angle_wall_raycasts=2 * np.pi,\n",
    "            world_bounds=([-50, -50], [50, 50]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 1 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1500, 2)\n",
      "(2, 1500, 3)\n",
      "Calculating views from 1 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of IoDataset:\n",
      "The first 3 dimensions are reduced from (1, 2, 1500) to (3000)\n",
      "actions\t(3000, 2):\tconsisting of speed [cm/s] and turn [rad/s].\n",
      "actions_binned\t(3000, 2):\tconsisting of speed [discretized into 20 bins] and turn [discretized into 20 bins]\n",
      "views\t(3000, 10):\t5 fish_bins and 5 wall ray casts.\n",
      "\n",
      "Loading data from 1 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1500, 2)\n",
      "(2, 1500, 3)\n",
      "Calculating views from 1 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of IoDataset:\n",
      "The first 3 dimensions are reduced from (1, 2, 1500) to (3000)\n",
      "actions\t(3000, 2):\tconsisting of speed [cm/s] and turn [rad/s].\n",
      "actions_binned\t(3000, 2):\tconsisting of speed [discretized into 20 bins] and turn [discretized into 20 bins]\n",
      "views\t(3000, 10):\t5 fish_bins and 5 wall ray casts.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset. When using actions_binned, the borders are neccessary.\n",
    "dset = fish_models.datasets.io_dataset.IoDataset(\n",
    "    data_folder,\n",
    "    raycast,\n",
    "    output_strings=[\"actions\", \"actions_binned\", \"views\"],\n",
    "    reduce_dim=2,\n",
    "    max_files=n_files,\n",
    "    max_timesteps=n_timesteps,\n",
    "    speed_bin_borders=speed_bins,\n",
    "    turn_bin_borders=turn_bins,\n",
    ")\n",
    "test_dset = fish_models.datasets.io_dataset.IoDataset(\n",
    "    test_data_folder,\n",
    "    raycast,\n",
    "    output_strings=[\"actions\", \"actions_binned\", \"views\"],\n",
    "    reduce_dim=2,\n",
    "    max_files=n_files,\n",
    "    max_timesteps=n_timesteps,\n",
    "    speed_bin_borders=speed_bins,\n",
    "    turn_bin_borders=turn_bins,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.deep_model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/ 1 Step: 46/47 Loss:   6.550686 \n",
      "Finished Training\n",
      "\n",
      "Training took 3.3423380851745605s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.train(dset, test_dset, optimizer, criterion, max_epochs=1)\n",
    "end = time.time()\n",
    "print(f'\\nTraining took {end - start}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.1996, 0.0000, 0.0000, 0.0000, 0.9391, 0.2954, 0.4228,\n",
      "          0.9957, 0.9391]]])\n",
      "torch.Size([1, 1, 10])\n",
      "tensor([[0.0000, 0.1996, 0.0000, 0.0000, 0.0000, 0.9391, 0.2954, 0.4228, 0.9957,\n",
      "         0.9391]])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ee9b103a1783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_track\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_guppies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfishes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackset_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_timesteps_simulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_io_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/fish_models/src/fish_models/gym_interface.py\u001b[0m in \u001b[0;36mcreate_track\u001b[0;34m(self, n_guppies, trackset_len, initial_poses, verbose, validation_dataset, histories)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackset_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;31m# Note: env.step is NOT one simulation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0mtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/robofish/gym_guppy/envs/_guppy_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_guppy_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__agents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/robofish/gym_guppy/envs/_guppy_env.py\u001b[0m in \u001b[0;36m_compute_guppy_actions\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_kdtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguppies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_next_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkd_tree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkd_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step_world\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/fish_models/src/fish_models/gym_interface.py\u001b[0m in \u001b[0;36mcompute_next_action\u001b[0;34m(self, state, kd_tree, verbose)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{parameter}' not a possible parameter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mspeed_cm_per_second\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn_rad_per_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Test if real speed turn is working\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-d07fbae45716>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, view)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mspeed_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mturn_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed_bins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-d07fbae45716>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, view)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#output = self.deep_model(torch.tensor(np.array([view]).astype(np.double)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-66b49ee9f474>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Robofish-IwTRahcW/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 294\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    295\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "generator = fish_models.gym_interface.TrackGeneratorGym(\n",
    "    model, [100,100], 25, raycast=raycast\n",
    ")\n",
    "\n",
    "track = generator.create_track(n_guppies=fishes, trackset_len=n_timesteps_simulation)\n",
    "\n",
    "f = generator.as_io_file(track)\n",
    "f.save_as(\"output/resnet.hdf5\")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim(-50,50)\n",
    "plt.ylim(-50,50)\n",
    "for fish_id in range(fishes):\n",
    "    plt.plot(track[fish_id, :, 0], track[fish_id, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNetFischModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
